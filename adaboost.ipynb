{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-01T17:11:30.327313Z","iopub.execute_input":"2023-03-01T17:11:30.328264Z","iopub.status.idle":"2023-03-01T17:11:33.583832Z","shell.execute_reply.started":"2023-03-01T17:11:30.328146Z","shell.execute_reply":"2023-03-01T17:11:33.582616Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Resources: \n* https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/\n* sklearn documentation\n* Multi class adaboost Ji Zhu\n* AdaBoost-CNN Aboozar","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\nimport cv2\n\nimport PIL.Image as Image\nimport os\n\nimport matplotlib.pylab as plt\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.optimizers.experimental import RMSprop, Adam\nimport gc\nimport random\nimport math\nimport keras.backend as K\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\n\n\n#import libraries\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential , Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Input, Add, Activation, ZeroPadding2D, BatchNormalization, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D , Dense, Conv2D, MaxPool2D , Flatten , concatenate ,DepthwiseConv2D, GlobalAveragePooling2D , ReLU","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:11:33.588692Z","iopub.execute_input":"2023-03-01T17:11:33.589066Z","iopub.status.idle":"2023-03-01T17:11:41.661188Z","shell.execute_reply.started":"2023-03-01T17:11:33.589007Z","shell.execute_reply":"2023-03-01T17:11:41.659942Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Read Data","metadata":{}},{"cell_type":"code","source":"def read_data(list, photo_size):  \n    data = []\n    for i in range(len(list)):\n        for filename in os.listdir(list[i]):\n            img  = cv2.imread(list[i] + filename)\n            img = cv2.resize(img,(photo_size,photo_size))\n            data.append([img, [i%2, abs(i%2 - 1)] ])\n        \n    \n    random.shuffle(data)\n    x = []\n    y = []\n    for i in range(len(data)):\n        x.append(data[i][0])\n        y.append(data[i][1])\n    \n    x = np.array(x)\n    y = np.array(y)\n    x = x / 255\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:11:41.666498Z","iopub.execute_input":"2023-03-01T17:11:41.669234Z","iopub.status.idle":"2023-03-01T17:11:41.679789Z","shell.execute_reply.started":"2023-03-01T17:11:41.669189Z","shell.execute_reply":"2023-03-01T17:11:41.677846Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Xception","metadata":{}},{"cell_type":"code","source":"def make_model():\n    xception = tf.keras.applications.Xception(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\" )\n    \n    model = tf.keras.models.Sequential([\n    xception,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(128, \"relu\"),\n    tf.keras.layers.Dense(64, \"relu\"),\n    tf.keras.layers.Dense(2, \"softmax\")\n    ])\n      \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:11:41.683039Z","iopub.execute_input":"2023-03-01T17:11:41.683498Z","iopub.status.idle":"2023-03-01T17:11:41.694290Z","shell.execute_reply.started":"2023-03-01T17:11:41.683458Z","shell.execute_reply":"2023-03-01T17:11:41.693360Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Data split","metadata":{}},{"cell_type":"code","source":"ta = \"/kaggle/input/autistic-children-facial-data-set/train/autistic/\"\ntn = \"/kaggle/input/autistic-children-facial-data-set/train/non_autistic/\"\ntesta = \"/kaggle/input/autistic-children-facial-data-set/test/autistic/\"\ntestn = \"/kaggle/input/autistic-children-facial-data-set/test/non_autistic/\"\nva = \"/kaggle/input/autistic-children-facial-data-set/valid/autistic/\"\nvn = \"/kaggle/input/autistic-children-facial-data-set/valid/non_autistic/\"\n\nlis = [tn, ta , testn, testa, vn, va]\n\n\nx, y = read_data(lis , 224)\nprint(x.shape)\nprint(y.shape)\n\ndel ta , tn , testa, testn, va, vn, lis\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:11:41.695460Z","iopub.execute_input":"2023-03-01T17:11:41.696151Z","iopub.status.idle":"2023-03-01T17:12:02.542884Z","shell.execute_reply.started":"2023-03-01T17:11:41.696116Z","shell.execute_reply":"2023-03-01T17:12:02.541840Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(2936, 224, 224, 3)\n(2936, 2)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"52"},"metadata":{}}]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.85, test_size = 0.15, stratify = y)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size = 0.9, test_size = 0.1, stratify = y_train)\ndel x, y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:12:02.544743Z","iopub.execute_input":"2023-03-01T17:12:02.545466Z","iopub.status.idle":"2023-03-01T17:12:05.186140Z","shell.execute_reply.started":"2023-03-01T17:12:02.545426Z","shell.execute_reply":"2023-03-01T17:12:05.185041Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(x_valid.shape)\nprint(y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:12:05.187409Z","iopub.execute_input":"2023-03-01T17:12:05.187857Z","iopub.status.idle":"2023-03-01T17:12:05.193952Z","shell.execute_reply.started":"2023-03-01T17:12:05.187820Z","shell.execute_reply":"2023-03-01T17:12:05.192960Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(2245, 224, 224, 3)\n(2245, 2)\n(441, 224, 224, 3)\n(441, 2)\n(250, 224, 224, 3)\n(250, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### MoibleNet","metadata":{}},{"cell_type":"code","source":"def depth_block(x, strides):\n    x = DepthwiseConv2D(3,strides=strides,padding='same',  use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    return x\ndef single_conv_block(x,filters):\n    x = Conv2D(filters, 1,use_bias=False)(x)\n    x= BatchNormalization()(x)\n    x = ReLU()(x)\n    return x\ndef combo_layer(x,filters,strides):\n    x = depth_block(x,strides)\n    x = single_conv_block(x, filters)\n    return x\ndef MobileNet(input_shape=(224,224,3),n_classes = 2):\n    input =layers.Input( input_shape)\n    x = Conv2D(32,3,strides=(2,2),padding = 'same', use_bias=False) (input)\n    x =  BatchNormalization()(x)\n    x = ReLU()(x)\n    x = combo_layer(x,64, strides=(1,1))\n    x = combo_layer(x,128,strides=(2,2))\n    x = combo_layer(x,128,strides=(1,1))\n    x = combo_layer(x,256,strides=(2,2))\n    x = combo_layer(x,256,strides=(1,1))\n    x = combo_layer(x,512,strides=(2,2))\n    for _ in range(5):\n        x = combo_layer(x,512,strides=(1,1))\n    x = combo_layer(x,1024,strides=(2,2))\n    x = combo_layer(x,1024,strides=(1,1))\n    x = GlobalAveragePooling2D()(x)\n    output = Dense(n_classes,activation='softmax')(x)\n    model = Model(input, output)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:12:05.195519Z","iopub.execute_input":"2023-03-01T17:12:05.196211Z","iopub.status.idle":"2023-03-01T17:12:05.209284Z","shell.execute_reply.started":"2023-03-01T17:12:05.196177Z","shell.execute_reply":"2023-03-01T17:12:05.208329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# SAMME Algorithm","metadata":{}},{"cell_type":"code","source":"def samme_evalution(x,y,n_e, estimators_alphas, estimators):\n    predicted = []\n    for i in range(n_e):\n        predicted.append(estimators[i].predict(x))\n        gc.collect()\n    _m5ra = []   \n    for i in range(n_e):\n        _m5ra.append([estimators_alphas[i] * j for j in predicted[i].round()])\n    _m5ra = np.array(_m5ra).sum(axis = 0)\n\n    acc = 0\n    for i in range(len(x)):\n       if (np.argmax(_m5ra[i])) == (np.argmax(y[i])):\n            acc += 1 \n    return acc / y.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-02-26T18:48:56.815552Z","iopub.execute_input":"2023-02-26T18:48:56.816556Z","iopub.status.idle":"2023-02-26T18:48:56.824867Z","shell.execute_reply.started":"2023-02-26T18:48:56.816521Z","shell.execute_reply":"2023-02-26T18:48:56.823121Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"estimators = []  # list of models\nestimators_weights = [] # list of CNN weights of each model\nestimators_alphas = [] # \nsample_weights = [] # list of sample weights of each model\nn_e = 6 # number of models\ngc.collect()\n\nfor i in range(0, n_e):\n    gc.collect()\n    # models after base\n    if i > 0 :\n        estimator = make_model()\n        gc.collect()\n        estimator.set_weights(estimators_weights[i-1])\n        gc.collect()\n        estimator.compile(optimizer=\"RMSprop\",loss=tf.keras.losses.BinaryCrossentropy(),metrics=['acc'])\n        gc.collect()\n        sample_weight = sample_weights[i-1]\n        gc.collect()\n    # base model\n    else:\n        estimator = make_model()\n        gc.collect()\n        estimator.compile(optimizer=\"RMSprop\",loss=tf.keras.losses.BinaryCrossentropy(),metrics=['acc'])\n        gc.collect()\n        sample_weight = np.ones( x_train.shape[0] ) / x_train.shape[0]\n        gc.collect()\n    \n        \n    print(f\"Estimator no. {i+1} \\n\\n\")\n    gc.collect()\n    gc.collect()\n    \n    # base model and others could have different epochs\n    if i > 0:\n        estimator.fit(x_train, y_train, sample_weight = sample_weight, validation_data = [x_valid , y_valid], epochs = 1, batch_size = 32)\n    else:\n        estimator.fit(x_train, y_train, sample_weight = sample_weight, validation_data = [x_valid , y_valid], epochs = 1, batch_size = 32)\n    gc.collect()\n    gc.collect()\n    \n    # simple evaluation for each model\n    print(\"\\n\\n\")\n    print(f\"Train Acc: {estimator.evaluate(x_train, y_train)}\\n\\n\")\n    gc.collect()\n    gc.collect()\n    print(f\"Test Acc: {estimator.evaluate(x_test, y_test)}\\n\\n\")\n    gc.collect()\n    gc.collect()\n    print(f\"Valid Acc: {estimator.evaluate(x_valid, y_valid)}\\n\\n\")\n    gc.collect()\n    gc.collect()\n    \n    \n    # Step 2: Calculate the err equation\n    #print(\"start step 2\")\n    predicted = (estimator.predict(x_train)).round()\n    gc.collect()\n    gc.collect()\n    \n    # 1 if the model classifiy it wrong, 0 then it is calssified it right\n    incorrect = np.array([1 if i.all() == True else 0 for i in predicted != y_train ])\n    del predicted\n    gc.collect()\n    \n    error = np.dot(incorrect, sample_weight) / np.sum(sample_weight)\n    #print(f\"err = {error}\\n\\n\")\n    gc.collect()\n\n    ############ we need to change this condition  ###############\n    if error >= 1 - 1 / 2 and i > 0:\n        print(\"stoped boosting\")\n        break\n    \n    #print(\"start step 3\")\n    # step 3: caluclate the alpha\n    lr = K.eval(estimator.optimizer.lr)\n    alpha = lr*(math.log((1-error)/error)) + math.log(2-1)\n    del error, lr\n    gc.collect()\n    \n    #print(\"start step 4\")\n    # step 4: update the weights\n    sample_weight *= np.exp(alpha * incorrect) \n    gc.collect()\n    sample_weight /= sample_weight.sum()\n    \n    # appending and cleaning before repeating\n    estimators_alphas.append(alpha)\n    del alpha , incorrect\n    gc.collect()\n    \n    estimators.append(estimator)\n    gc.collect()\n    \n    estimators_weights.append(estimator.get_weights())\n    del estimator\n    gc.collect()\n    \n    sample_weights.append(sample_weight)\n    del sample_weight\n    gc.collect()\n    \n    #print(\"finish \\n\")\n\ndel estimators_weights\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T18:51:31.740497Z","iopub.execute_input":"2023-02-26T18:51:31.740874Z","iopub.status.idle":"2023-02-26T18:59:19.337346Z","shell.execute_reply.started":"2023-02-26T18:51:31.740842Z","shell.execute_reply":"2023-02-26T18:59:19.336166Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":269,"outputs":[{"name":"stdout","text":"Estimator no. 1 \n\n\n71/71 [==============================] - 36s 397ms/step - loss: 2.4479e-04 - acc: 0.7461 - val_loss: 1.4362 - val_acc: 0.7360\n\n\n\n71/71 [==============================] - 6s 79ms/step - loss: 1.0727 - acc: 0.7915\nTrain Acc: [1.072749137878418, 0.7915367484092712]\n\n\n14/14 [==============================] - 1s 78ms/step - loss: 1.5073 - acc: 0.7188\nTest Acc: [1.507330298423767, 0.7188208699226379]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 1.4362 - acc: 0.7360\nValid Acc: [1.4361693859100342, 0.7360000014305115]\n\n\n71/71 [==============================] - 6s 75ms/step\nEstimator no. 2 \n\n\n71/71 [==============================] - 37s 401ms/step - loss: 1.5731e-04 - acc: 0.8508 - val_loss: 0.7050 - val_acc: 0.8440\n\n\n\n71/71 [==============================] - 6s 78ms/step - loss: 0.3223 - acc: 0.9158\nTrain Acc: [0.32231205701828003, 0.9158129096031189]\n\n\n14/14 [==============================] - 1s 77ms/step - loss: 0.9051 - acc: 0.8050\nTest Acc: [0.9051039218902588, 0.80498868227005]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 0.7050 - acc: 0.8440\nValid Acc: [0.704990565776825, 0.843999981880188]\n\n\n71/71 [==============================] - 6s 76ms/step\nEstimator no. 3 \n\n\n71/71 [==============================] - 36s 400ms/step - loss: 9.5455e-05 - acc: 0.9207 - val_loss: 0.5466 - val_acc: 0.8560\n\n\n\n71/71 [==============================] - 6s 78ms/step - loss: 0.1602 - acc: 0.9599\nTrain Acc: [0.16015420854091644, 0.9599109292030334]\n\n\n14/14 [==============================] - 1s 80ms/step - loss: 0.9206 - acc: 0.8141\nTest Acc: [0.9205577373504639, 0.8140589594841003]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 0.5466 - acc: 0.8560\nValid Acc: [0.546632707118988, 0.8560000061988831]\n\n\n71/71 [==============================] - 6s 78ms/step\nEstimator no. 4 \n\n\n71/71 [==============================] - 36s 400ms/step - loss: 6.0786e-05 - acc: 0.9506 - val_loss: 1.4843 - val_acc: 0.7480\n\n\n\n71/71 [==============================] - 6s 79ms/step - loss: 0.7064 - acc: 0.8388\nTrain Acc: [0.7063701748847961, 0.838752806186676]\n\n\n14/14 [==============================] - 1s 78ms/step - loss: 1.3694 - acc: 0.7370\nTest Acc: [1.3693773746490479, 0.7369614243507385]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 1.4843 - acc: 0.7480\nValid Acc: [1.4842619895935059, 0.7480000257492065]\n\n\n71/71 [==============================] - 6s 75ms/step\nEstimator no. 5 \n\n\n71/71 [==============================] - 36s 400ms/step - loss: 5.4712e-05 - acc: 0.9612 - val_loss: 0.4964 - val_acc: 0.8600\n\n\n\n71/71 [==============================] - 6s 79ms/step - loss: 0.1187 - acc: 0.9608\nTrain Acc: [0.11865829676389694, 0.9608017802238464]\n\n\n14/14 [==============================] - 1s 78ms/step - loss: 0.9126 - acc: 0.8118\nTest Acc: [0.9126170873641968, 0.8117913603782654]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 0.4964 - acc: 0.8600\nValid Acc: [0.49640461802482605, 0.8600000143051147]\n\n\n71/71 [==============================] - 6s 76ms/step\nEstimator no. 6 \n\n\n71/71 [==============================] - 36s 401ms/step - loss: 4.2084e-05 - acc: 0.9661 - val_loss: 2.3630 - val_acc: 0.7720\n\n\n\n71/71 [==============================] - 6s 80ms/step - loss: 0.7490 - acc: 0.8935\nTrain Acc: [0.7489763498306274, 0.8935412168502808]\n\n\n14/14 [==============================] - 1s 79ms/step - loss: 2.6600 - acc: 0.7370\nTest Acc: [2.6599700450897217, 0.7369614243507385]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 2.3630 - acc: 0.7720\nValid Acc: [2.362954616546631, 0.7720000147819519]\n\n\n71/71 [==============================] - 6s 76ms/step\n","output_type":"stream"},{"execution_count":269,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"print(f\" Train Acc: {samme_evalution(x_train,y_train,n_e, estimators_alphas, estimators)}\")\nprint(f\" Test Acc: {samme_evalution(x_test,y_test,n_e, estimators_alphas, estimators)}\")\nprint(f\" Validation Acc: {samme_evalution(x_valid,y_valid,n_e, estimators_alphas, estimators)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-26T18:59:19.339517Z","iopub.execute_input":"2023-02-26T18:59:19.339916Z","iopub.status.idle":"2023-02-26T19:00:47.789633Z","shell.execute_reply.started":"2023-02-26T18:59:19.339879Z","shell.execute_reply":"2023-02-26T19:00:47.787786Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":270,"outputs":[{"name":"stdout","text":"71/71 [==============================] - 5s 76ms/step\n71/71 [==============================] - 5s 75ms/step\n71/71 [==============================] - 5s 77ms/step\n71/71 [==============================] - 5s 75ms/step\n71/71 [==============================] - 5s 76ms/step\n71/71 [==============================] - 5s 77ms/step\n Train Acc: 0.9848552338530067\n14/14 [==============================] - 1s 75ms/step\n14/14 [==============================] - 1s 75ms/step\n14/14 [==============================] - 1s 74ms/step\n14/14 [==============================] - 1s 74ms/step\n14/14 [==============================] - 1s 74ms/step\n14/14 [==============================] - 1s 75ms/step\n Test Acc: 0.8253968253968254\n8/8 [==============================] - 1s 75ms/step\n8/8 [==============================] - 1s 73ms/step\n8/8 [==============================] - 1s 73ms/step\n8/8 [==============================] - 1s 74ms/step\n8/8 [==============================] - 1s 75ms/step\n8/8 [==============================] - 1s 74ms/step\n Validation Acc: 0.86\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Notes\n","metadata":{}},{"cell_type":"markdown","source":"# SAMME.R","metadata":{}},{"cell_type":"code","source":"# Step 3: Calculate h\ndef Hk(predicted, k = 2):\n    predicted = np.array(predicted)\n    predicted[predicted < np.finfo(predicted.dtype).eps] = np.finfo(predicted.dtype).eps\n    log_predict = np.log(predicted)\n    h = (k-1) * (log_predict - 1/k * log_predict.sum())\n    \n    return h\n\ndef evaluate(x, y, estimators, n_e, hk = None, k = 2):\n    if hk == None:\n        hk = []\n        for i in range(0,n_e):\n            hk.append(estimators[i].predict(x))\n            gc.collect()\n\n        hk = np.array(hk)\n        hk[hk < np.finfo(hk.dtype).eps] = np.finfo(hk.dtype).eps\n        gc.collect()\n        log_prop = np.log(hk)\n        gc.collect()\n        intermediate = (k-1) * (log_prop - 1/k * log_prop.sum())\n        gc.collect()\n        intermediate = intermediate.sum(axis = 0)\n    else:\n        intermediate = np.array(hk).sum(axis = 0)\n    acc = 0\n    for i in range(len(y)):\n       if (np.argmax(intermediate[i])) == (np.argmax(y[i])):\n            acc += 1 \n    print(f\"Accuracy : {acc/y.shape[0]}\")\n    return acc/y.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:39:13.018214Z","iopub.execute_input":"2023-03-01T17:39:13.018605Z","iopub.status.idle":"2023-03-01T17:39:13.030264Z","shell.execute_reply.started":"2023-03-01T17:39:13.018571Z","shell.execute_reply":"2023-03-01T17:39:13.029269Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ngc.collect()\n\nestimators = [] # list of models\nestimators_weights = [] # list of CNN weights of each model\nsample_weights = [] # list of sample weights of each model\n#H = [] # list of h of each model\n\nestimators_pred = []\n\ny_labels = [ i[0] for i in y_train]\ny_coding = np.array([1 if i == 1 else 1.01 for i in y_labels])\n\nn_e = 4\ngc.collect()\n\nfor i in range(0, n_e):\n    gc.collect()\n    # models after base\n    if i > 0 :\n        estimator = make_model() #MobileNet()\n        gc.collect()\n        estimator.set_weights(estimators_weights[i-1])\n        gc.collect()\n        estimator.compile(optimizer=\"RMSprop\",loss=tf.keras.losses.BinaryCrossentropy(),metrics=['acc'])\n        gc.collect()\n        sample_weight = sample_weights[i-1]\n        gc.collect()\n    \n    # base model\n    else:\n        estimator = make_model() #MobileNet()\n        gc.collect()\n        estimator.compile(optimizer=\"RMSprop\",loss=tf.keras.losses.BinaryCrossentropy(),metrics=['acc'])\n        gc.collect()\n        sample_weight = np.ones(x_train.shape[0]) / x_train.shape[0]\n        gc.collect()\n    \n        \n    print(f\"Estimator no. {i+1} \\n\\n\")\n    gc.collect()\n    gc.collect()\n    \n    # base model and others could have different epochs\n    if i > 0:\n        estimator.fit(x_train, y_train, sample_weight=sample_weight, validation_data = [x_valid , y_valid], epochs = 1, batch_size = 64)\n    else:\n        estimator.fit(x_train, y_train, sample_weight=sample_weight, validation_data = [x_valid , y_valid], epochs = 2, batch_size = 8)\n    gc.collect()\n    gc.collect()\n    \n    # simple evaluation for each model\n    print(\"\\n\\n\")\n    print(f\"Train Acc: {estimator.evaluate(x_train, y_train)}\\n\\n\")\n    gc.collect()\n    gc.collect()\n    print(f\"Test Acc: {estimator.evaluate(x_test, y_test)}\\n\\n\")\n    gc.collect()\n    gc.collect()\n    print(f\"Valid Acc: {estimator.evaluate(x_valid, y_valid)}\\n\\n\")\n    gc.collect()\n    gc.collect()\n    \n    ############# first we need to calculate the alpha for early stopping ############\n    #################################################################################\n    ################### DONT, DONT, DONT FOOOOOORGEEEEEEET #########################\n    ################### DONT, DONT, DONT FOOOOOORGEEEEEEET #########################\n    ###############################################################################\n    \n    # Step 2: get class predicted prop\n    #print(\"start step 2\")\n    predicted = (estimator.predict(x_train))\n    gc.collect()\n    gc.collect()\n    \n    prop = np.array([predicted[i][0] for i in range(len(y_train))])\n    prop[prop < np.finfo(prop.dtype).eps] = np.finfo(prop.dtype).eps\n    gc.collect()\n    gc.collect()\n    \n    # Step 3: Calculate h\n    #predicted = np.array(predicted)\n    #H.append(Hk(predicted))\n    #del predicted\n    #gc.collect()\n    #gc.collect()\n    \n    #print(\"start step 4\")\n    # step 4: update the weights\n    lr =  1 #K.eval(estimator.optimizer.lr)\n    intermediate_value = np.exp(-lr * (((2 - 1) / 2) * (y_coding* np.log(prop))))\n    gc.collect()\n    sample_weight = sample_weight * intermediate_value\n    gc.collect()\n    sample_weight = sample_weight / sample_weight.sum()\n    del intermediate_value, prop, lr\n    gc.collect()\n    \n    # appending and cleaning before repeating\n    estimators.append(estimator)\n    gc.collect()\n    \n    estimators_weights.append(estimator.get_weights())\n    del estimator\n    gc.collect()\n    \n    sample_weights.append(sample_weight)\n    del sample_weight\n    gc.collect()\n    #print(\"finish \\n\")\n\ndel estimators_weights\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:39:13.977553Z","iopub.execute_input":"2023-03-01T17:39:13.977901Z","iopub.status.idle":"2023-03-01T17:45:28.902244Z","shell.execute_reply.started":"2023-03-01T17:39:13.977870Z","shell.execute_reply":"2023-03-01T17:45:28.901305Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Estimator no. 1 \n\n\nEpoch 1/2\n281/281 [==============================] - 44s 128ms/step - loss: 2.7808e-04 - acc: 0.6592 - val_loss: 8.0220 - val_acc: 0.6160\nEpoch 2/2\n281/281 [==============================] - 35s 123ms/step - loss: 2.1919e-04 - acc: 0.7813 - val_loss: 0.4460 - val_acc: 0.7960\n\n\n\n71/71 [==============================] - 6s 78ms/step - loss: 0.4085 - acc: 0.8178\nTrain Acc: [0.40847888588905334, 0.8178173899650574]\n\n\n14/14 [==============================] - 1s 78ms/step - loss: 0.5031 - acc: 0.7619\nTest Acc: [0.5031496286392212, 0.761904776096344]\n\n\n8/8 [==============================] - 1s 77ms/step - loss: 0.4460 - acc: 0.7960\nValid Acc: [0.4459714889526367, 0.7960000038146973]\n\n\n71/71 [==============================] - 6s 75ms/step\nEstimator no. 2 \n\n\n36/36 [==============================] - 35s 765ms/step - loss: 1.9703e-05 - acc: 0.8673 - val_loss: 0.4036 - val_acc: 0.8400\n\n\n\n71/71 [==============================] - 6s 79ms/step - loss: 0.2849 - acc: 0.8846\nTrain Acc: [0.28489986062049866, 0.8846325278282166]\n\n\n14/14 [==============================] - 1s 78ms/step - loss: 0.4544 - acc: 0.8163\nTest Acc: [0.45437097549438477, 0.8163265585899353]\n\n\n8/8 [==============================] - 1s 76ms/step - loss: 0.4036 - acc: 0.8400\nValid Acc: [0.4035740792751312, 0.8399999737739563]\n\n\n71/71 [==============================] - 6s 75ms/step\nEstimator no. 3 \n\n\n36/36 [==============================] - 36s 773ms/step - loss: 7.4322e-08 - acc: 0.8806 - val_loss: 0.4189 - val_acc: 0.8440\n\n\n\n71/71 [==============================] - 6s 78ms/step - loss: 0.2698 - acc: 0.8882\nTrain Acc: [0.2698058485984802, 0.8881959915161133]\n\n\n14/14 [==============================] - 1s 78ms/step - loss: 0.4640 - acc: 0.8163\nTest Acc: [0.4640287458896637, 0.8163265585899353]\n\n\n8/8 [==============================] - 1s 79ms/step - loss: 0.4189 - acc: 0.8440\nValid Acc: [0.41891244053840637, 0.843999981880188]\n\n\n71/71 [==============================] - 6s 75ms/step\nEstimator no. 4 \n\n\n36/36 [==============================] - 35s 765ms/step - loss: 1.7093e-08 - acc: 0.8855 - val_loss: 0.4263 - val_acc: 0.8560\n\n\n\n8/8 [==============================] - 1s 76ms/step - loss: 0.4263 - acc: 0.8560\nValid Acc: [0.4263460338115692, 0.8560000061988831]\n\n\n71/71 [==============================] - 6s 76ms/step\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"print(f\" Train Acc: {evaluate(x_train, y_train, estimators, n_e, k = 2)}\")\nprint(f\" Test Acc: {evaluate(x_test, y_test, estimators, n_e, k = 2)}\")\nprint(f\" Validation Acc: {evaluate(x_valid, y_valid, estimators, n_e, k = 2)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:45:28.905949Z","iopub.execute_input":"2023-03-01T17:45:28.906247Z","iopub.status.idle":"2023-03-01T17:46:24.536494Z","shell.execute_reply.started":"2023-03-01T17:45:28.906222Z","shell.execute_reply":"2023-03-01T17:46:24.535389Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"71/71 [==============================] - 5s 76ms/step\n71/71 [==============================] - 5s 76ms/step\n71/71 [==============================] - 5s 76ms/step\n71/71 [==============================] - 5s 76ms/step\nAccuracy : 0.8832962138084632\n Train Acc: 0.8832962138084632\n14/14 [==============================] - 1s 74ms/step\n14/14 [==============================] - 1s 75ms/step\n14/14 [==============================] - 1s 75ms/step\n14/14 [==============================] - 1s 75ms/step\nAccuracy : 0.81859410430839\n Test Acc: 0.81859410430839\n8/8 [==============================] - 1s 74ms/step\n8/8 [==============================] - 1s 75ms/step\n8/8 [==============================] - 1s 79ms/step\n8/8 [==============================] - 1s 75ms/step\nAccuracy : 0.848\n Validation Acc: 0.848\n","output_type":"stream"}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T17:17:59.436064Z","iopub.status.idle":"2023-03-01T17:17:59.436968Z","shell.execute_reply.started":"2023-03-01T17:17:59.436697Z","shell.execute_reply":"2023-03-01T17:17:59.436723Z"},"trusted":true},"execution_count":null,"outputs":[]}]}